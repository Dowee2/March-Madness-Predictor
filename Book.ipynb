{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install --upgrade pandas\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%cd data/Mens/Season/\n",
    "games_df = pd.DataFrame()\n",
    "for season in range(2003 , 2025):\n",
    "    season_games = pd.read_csv(f'{season}/MRegularSeasonDetailedResults_{season}.csv')\n",
    "    games_df = pd.concat([games_df, season_games])\n",
    "\n",
    "ordinal_df = pd.DataFrame()\n",
    "for season in range(2003 , 2025):\n",
    "    ordinal_games = pd.read_csv(f'{season}/MMasseyOrdinals_{season}.csv')\n",
    "    ordinal_df = pd.concat([ordinal_df, ordinal_games])\n",
    "\n",
    "# ordinal_df = pd.read_csv('2024/MMasseyOrdinals_2024.csv')\n",
    "# games_df = pd.read_csv('2024/MRegularSeasonDetailedResults_2024.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# games_df['Week'] = ((games_df['DayNum']-1)/7 +1)\n",
    "# games_df['Week'] = games_df['Week'].apply(np.floor)\n",
    "\n",
    "\n",
    "# ordinal_df['Week'] = ((ordinal_df['RankingDayNum']-1)/7 +1)\n",
    "# ordinal_df['Week'] = ordinal_df['Week'].apply(np.floor)\n",
    "# games_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_additional_stats(df):\n",
    "    \"\"\"\n",
    "    Adds calculated statistics for two-point field goals to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The original game results DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The modified DataFrame with additional stats.\n",
    "    \"\"\"\n",
    "    df['WFGM2'] = df['WFGM'] - df['WFGM3']\n",
    "    df['WFGA2'] = df['WFGA'] - df['WFGA3']\n",
    "    df['LFGM2'] = df['LFGM'] - df['LFGM3']\n",
    "    df['LFGA2'] = df['LFGA'] - df['LFGA3']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_team_stats(df):\n",
    "    \"\"\"\n",
    "    Prepares and aggregates team statistics and statistics against from game results.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The game results DataFrame with additional stats.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame with average stats per team and stats against.\n",
    "    \"\"\"\n",
    "    df = calculate_additional_stats(df)\n",
    "   # Stats when the team wins\n",
    "    win_stats = df[['Season','WTeamID', 'WFGM', 'WFGA', 'WFGM2', 'WFGA2', 'WFGM3', 'WFGA3', 'WFTM',\n",
    "                    'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']].copy()\n",
    "    win_stats.columns = ['Season','TeamID', 'FGM', 'FGA', 'FGM2', 'FGA2', 'FGM3', 'FGA3', 'FTM',\n",
    "                         'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n",
    "\n",
    "    # Stats against the team when it wins (opponents' performance)\n",
    "    win_against_stats = df[['Season','WTeamID', 'LFGM', 'LFGA', 'LFGM2', 'LFGA2', 'LFGM3', 'LFGA3', 'LFTM',\n",
    "                            'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']].copy()\n",
    "    win_against_stats.columns = ['Season','TeamID', 'FGM', 'FGA', 'FGM2', 'FGA2', 'FGM3', 'FGA3',\n",
    "                                  'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', \n",
    "                                  'Blk', 'PF']\n",
    "\n",
    "    # Stats when the team loses\n",
    "    lose_stats = df[['Season','LTeamID', 'LFGM', 'LFGA', 'LFGM2', 'LFGA2', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA',\n",
    "                     'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']].copy()\n",
    "    lose_stats.columns = ['Season','TeamID', 'FGM', 'FGA', 'FGM2', 'FGA2', 'FGM3', 'FGA3', 'FTM', 'FTA',\n",
    "                          'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n",
    "\n",
    "    # Stats against the team when it loses (opponents' performance)\n",
    "    lose_against_stats = df[['Season','LTeamID', 'WFGM', 'WFGA', 'WFGM2', 'WFGA2', 'WFGM3', 'WFGA3', 'WFTM',\n",
    "                             'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']].copy()\n",
    "    lose_against_stats.columns = ['Season','TeamID', 'FGM', 'FGA', 'FGM2', 'FGA2', 'FGM3', 'FGA3',\n",
    "                                  'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', \n",
    "                                  'Blk', 'PF']    \n",
    "\n",
    "    # Combine winning and losing stats\n",
    "    all_stats = pd.concat([win_stats, lose_stats])\n",
    "    all_against_stats = pd.concat([win_against_stats, lose_against_stats])\n",
    "\n",
    "    # Calculate the mean for stats and stats against separately\n",
    "    avg_stats = all_stats.groupby(['Season','TeamID']).mean().reset_index()\n",
    "    avg_against_stats = all_against_stats.groupby(['Season','TeamID']).mean().reset_index()\n",
    "\n",
    "    # Merge the average stats with average stats against\n",
    "    avg_merged_stats = pd.merge(avg_stats, avg_against_stats, on=['Season','TeamID'], suffixes=('', '_A'))\n",
    "    avg_merged_stats = avg_merged_stats.round(2)\n",
    "    return avg_merged_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# seasons = games_df['Season'].unique()\n",
    "# games_by_season = [games_df[games_df['Season'] == season] for season in seasons]\n",
    "    \n",
    "\n",
    "# teams_avg_stats = [prepare_team_stats(season) for season in games_by_season]\n",
    "team_avg_stats = prepare_team_stats(games_df)\n",
    "team_avg_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ordinal_df = ordinal_df.sort_values(by=['TeamID', 'RankingDayNum']).reset_index(drop=True)\n",
    "# ordinal_df = ordinal_df.rename(columns={'RankingDayNum':'DayNum'})\n",
    "\n",
    "# system_names = ordinal_df['SystemName'].unique()\n",
    "# teams_names = ordinal_df['TeamID'].unique()\n",
    "# system_no_rank_all_teams = []\n",
    "\n",
    "# for system in system_names:\n",
    "#     teams_in_system = ordinal_df[ordinal_df['SystemName'] == system]['TeamID'].unique()\n",
    "#     if len(teams_in_system) != len(teams_names):\n",
    "#         system_no_rank_all_teams.append(system)\n",
    "\n",
    "# ordinal_df = ordinal_df[~ordinal_df['SystemName'].isin(system_no_rank_all_teams)]\n",
    "# ordinal_pivot = ordinal_df.pivot_table(index=['TeamID', 'DayNum', 'Week'], columns='SystemName', values='OrdinalRank').reset_index()\n",
    "# ordinal_pivot.sort_values(by=['TeamID', 'DayNum'])\n",
    "# ordinal_pivot = ordinal_pivot.ffill()\n",
    "# ordinal_pivot = ordinal_pivot.groupby('TeamID').apply(lambda x: x.interpolate(method='linear', limit_direction='both')).reset_index(drop=True)\n",
    "\n",
    "# ordinal_df = ordinal_df.sort_values(by=['TeamID']).reset_index(drop=True)\n",
    "# ordinal_df = ordinal_df.drop(columns=['RankingDayNum'])\n",
    "\n",
    "# system_names = ordinal_df['SystemName'].unique()\n",
    "# teams_names = ordinal_df['TeamID'].unique()\n",
    "# system_no_rank_all_teams = []\n",
    "\n",
    "# for system in system_names:\n",
    "#     teams_in_system = ordinal_df[ordinal_df['SystemName'] == system]['TeamID'].unique()\n",
    "#     if len(teams_in_system) != len(teams_names):\n",
    "#         system_no_rank_all_teams.append(system)\n",
    "\n",
    "# ordinal_df = ordinal_df[~ordinal_df['SystemName'].isin(system_no_rank_all_teams)]\n",
    "# ordinal_pivot = ordinal_df.pivot_table(index=['TeamID'], columns='SystemName', values='OrdinalRank').reset_index()\n",
    "# ordinal_pivot.sort_values(by=['TeamID'])\n",
    "# ordinal_pivot = ordinal_pivot.groupby('TeamID').mean().reset_index()\n",
    "\n",
    "def prep_ordinal_ratings_for_merge(ordinal_df):\n",
    "    \"\"\"\n",
    "    Preprocesses the ordinal ratings dataframe for merging with other dataframes.\n",
    "    \n",
    "    Args:\n",
    "        ordinal_df (pandas.DataFrame): The ordinal ratings dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The preprocessed ordinal ratings dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ordinal_df = ordinal_df.sort_values(by=['Season','TeamID']).reset_index(drop=True)\n",
    "    ordinal_df = ordinal_df.drop(columns=['RankingDayNum'])\n",
    "\n",
    "    system_names = ordinal_df['SystemName'].unique()\n",
    "    teams_names = ordinal_df['TeamID'].unique()\n",
    "    system_no_rank_all_teams = []\n",
    "\n",
    "    for system in system_names:\n",
    "        teams_in_system = ordinal_df[ordinal_df['SystemName'] == system]['TeamID'].unique()\n",
    "        if len(teams_in_system) != len(teams_names):\n",
    "            system_no_rank_all_teams.append(system)\n",
    "\n",
    "    ordinal_df = ordinal_df[~ordinal_df['SystemName'].isin(system_no_rank_all_teams)]\n",
    "    ordinal_pivot = ordinal_df.pivot_table(index=['Season','TeamID'], columns='SystemName',\n",
    "                                           values='OrdinalRank').reset_index()\n",
    "    ordinal_pivot.sort_values(by=['Season','TeamID'])\n",
    "    ordinal_pivot = ordinal_pivot.ffill()\n",
    "    ordinal_pivot = ordinal_pivot.groupby(['Season','TeamID']).apply(lambda x: x.interpolate(method='linear', limit_direction='both')).reset_index(drop=True)\n",
    "    ordinal_pivot = ordinal_pivot.groupby(['Season','TeamID']).mean().reset_index()\n",
    "    \n",
    "    return ordinal_pivot\n",
    "\n",
    "\n",
    "# ordinal_pivot[(ordinal_pivot['TeamID'] == 1104)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinals_pivot = prep_ordinal_ratings_for_merge(ordinal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_avg_stats[(team_avg_stats['TeamID'] == 1104)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinals_pivot.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This code performs the following operations:\n",
    "\n",
    "# 1. Sorts the 'ordinal_pivot' DataFrame by 'TeamID' and 'DayNum' columns.\n",
    "# 2. Sorts the 'teams_stats_weekly_df' DataFrame by 'TeamID' and 'DayNum' columns.\n",
    "# 3. Merges the sorted 'teams_stats_weekly_df' and 'ordinal_pivot' DataFrames using 'DayNum' and 'TeamID' columns in a backward direction.\n",
    "# 4. Sorts the resulting DataFrame by 'TeamID' and 'DayNum' columns and resets the index.\n",
    "# 5. Extracts the columns starting from the 35th column and assigns them to 'rank_columns' variable.\n",
    "# 6. Fills the missing values in 'rank_columns' for each 'TeamID' using backward filling.\n",
    "# 7. Filters the resulting DataFrame to include only rows where 'TeamID' is equal to 1104.\n",
    "\n",
    "# Parameters:\n",
    "#     - ordinal_pivot: DataFrame containing ordinal rankings.\n",
    "#     - teams_stats_weekly_df: DataFrame containing weekly team statistics.\n",
    "\n",
    "# Returns:\n",
    "#     - weekly_stats_w_rating: DataFrame with sorted and merged data, filled with missing values, and filtered by 'TeamID' 1104.\n",
    "# \"\"\"\n",
    "# ordinals_pivot = ordinals_pivot.sort_values(by=['TeamID'])\n",
    "\n",
    "# teams_avg_stats = teams_avg_stats.sort_values(by=['TeamID'])\n",
    "# weekly_stats_w_rating = pd.merge(teams_avg_stats, ordinals_pivot, on='TeamID', suffixes=('', '_A'))\n",
    "# weekly_stats_w_rating = weekly_stats_w_rating.sort_values(by=['TeamID']).reset_index(drop=True)\n",
    "# rank_columns = weekly_stats_w_rating.columns[34:]\n",
    "\n",
    "# weekly_stats_w_rating[rank_columns] = weekly_stats_w_rating.groupby('TeamID')[rank_columns].bfill()\n",
    "# weekly_stats_w_rating = weekly_stats_w_rating.dropna(axis = 1, how= 'any')\n",
    "# weekly_stats_w_rating\n",
    "\n",
    "def merge_ratings_stats(ordinal_df, teams_stats_avg_df):\n",
    "    \"\"\"\n",
    "    Merge the ordinal dataframe and the teams' weekly stats \n",
    "    dataframe based on the 'TeamID' and 'DayNum' columns.\n",
    "    \n",
    "    Args:\n",
    "        ordinal_df (pandas.DataFrame): The ordinal dataframe containing the team ratings.\n",
    "        teams_stats_weekly_df (pandas.DataFrame): The teams' weekly stats dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The merged dataframe with the team ratings and weekly stats.\n",
    "    \"\"\"\n",
    "    ordinal_df = ordinal_df.sort_values(by=['TeamID'])\n",
    "\n",
    "    teams_stats_avg_df = teams_stats_avg_df.sort_values(by=['Season','TeamID'])\n",
    "    avg_stats_w_rating = pd.merge(teams_stats_avg_df, ordinal_df, on=['Season','TeamID'], suffixes=('', '_A'))\n",
    "    avg_stats_w_rating = avg_stats_w_rating.sort_values(by=['Season','TeamID']).reset_index(drop=True)\n",
    "    rank_columns = avg_stats_w_rating.columns[34:]\n",
    "\n",
    "    # Fill the NaN values in the rank columns with the previous values for each team. If still NaN, then drop the column.\n",
    "    avg_stats_w_rating[rank_columns] = avg_stats_w_rating.groupby(['Season','TeamID'])[rank_columns].bfill()\n",
    "    avg_stats_w_rating = avg_stats_w_rating.dropna(axis = 1, how= 'any')\n",
    "    return avg_stats_w_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stats = merge_ratings_stats(ordinals_pivot, team_avg_stats)\n",
    "merged_stats['Season'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_matchup_data(games_df, stats):\n",
    "    \"\"\"\n",
    "    Merges game data with team stats to prepare matchup data.\n",
    "\n",
    "    Parameters:\n",
    "    - games_df (DataFrame): The DataFrame containing game results.\n",
    "    - avg_stats (DataFrame): The DataFrame containing average stats per team.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Matchup data with team stats and game outcome.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for _, row in games_df.iterrows():\n",
    "        season = row['Season']\n",
    "        team_1, team_2 = sorted((row['WTeamID'], row['LTeamID']))\n",
    "        team_1_won = 1 if team_1 == row['WTeamID'] else 0\n",
    "        team_1_stats = stats.loc[(stats['TeamID'] == team_1)].add_prefix('team_1_').iloc[-1]\n",
    "        team_2_stats = stats.loc[(stats['TeamID'] == team_2)].add_prefix('team_2_').iloc[-1]\n",
    "\n",
    "        matchup_data = {\n",
    "            'Season': row['Season'],\n",
    "            'DayNum': row['DayNum'],\n",
    "            'team_1': team_1,\n",
    "            'team_2': team_2,\n",
    "            'team_1_won': team_1_won\n",
    "        }\n",
    "        matchup_data.update(team_1_stats)\n",
    "        matchup_data.update(team_2_stats)\n",
    "\n",
    "        processed_data.append(matchup_data)\n",
    "\n",
    "    return pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_data = []\n",
    "for season in merged_stats['Season'].unique():\n",
    "    season_games = games_df[games_df['Season'] == season]\n",
    "    season_stats = merged_stats[merged_stats['Season'] == season]\n",
    "    matchup_data.append(prepare_matchup_data(season_games, season_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat(matchup_data)\n",
    "test.isna().sum().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_model_scalar(model_param, model_name, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3270)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Fit the model\n",
    "    model_param.fit(X_train_scaled, y_train)\n",
    "    y_pred = model_param.predict(X_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{model_name} scalar accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model_param, model_name, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3270)\n",
    "\n",
    "    # sfs = SequentialFeatureSelector(model_param, n_features_to_select=10)\n",
    "    # sfs.fit(X_train, y_train)\n",
    "    # X_train = sfs.transform(X_train)\n",
    "    # X_test = sfs.transform(X_test)\n",
    "    model_param.fit(X_train, y_train)\n",
    "    y_pred = model_param.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{model_name} accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matchups = prepare_matchup_data(games_df, weekly_stats_w_rating)\n",
    "\n",
    "X = test.drop(['DayNum','team_1_won'], axis=1)\n",
    "y = test['team_1_won']\n",
    "\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=3270, max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(random_state=3270, n_estimators=200, max_depth=10, min_samples_split=10, n_jobs=-1),\n",
    "    'Logistic Regression': LogisticRegression(random_state=3270, max_iter=1000, penalty = None, solver = 'lbfgs', ),\n",
    "    'XGBoost': XGBClassifier(random_state = 3270, n_estimators = 100, max_depth = 3, learning_rate = 0.1, gamma = 0, subsample = 0.8, colsample_bytree = 0.8)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    fit_model_scalar(model, name, X, y)\n",
    "    fit_model(model, name, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(random_state=3270, max_iter=1000, penalty = None, solver = 'lbfgs')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3270)\n",
    "\n",
    "lgr.fit(X_train, y_train)\n",
    "y_pred = lgr.predict(X_test)\n",
    "#Show the predictions and the actual values side by side\n",
    "pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).tail(50)\n",
    "#Show the accuracy of the predictions for the last 50 games\n",
    "accuracy_score(y_test[len(y_test)-50:], y_pred[len(y_test)-50:])\n",
    "# accuracy_score(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasons_matchup_avgs_10_data = pd.DataFrame()\n",
    "# for season in range(2003 , 2025):\n",
    "#     season_games = pd.read_csv(f'{season}/MRegularSeasonDetailedResults_{season}_matchups_avg_w_rating.csv')\n",
    "#     seasons_matchup_avgs_10_data = pd.concat([seasons_matchup_avgs_10_data, season_games])\n",
    "\n",
    "seasons_matchup_avgs_10_data = test\n",
    "seasons = seasons_matchup_avgs_10_data['Season'].unique()\n",
    "data = seasons_matchup_avgs_10_data\n",
    "seasons_matchup_avgs_10_data = [seasons_matchup_avgs_10_data[seasons_matchup_avgs_10_data['Season'] == season] for season in seasons]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['Season'] == 2024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeSeriesSplit_by_season(seasons_data):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets by season. Model is trained on all data up to a certain season and tested on the next season until the last season. \n",
    "\n",
    "    Parameters:\n",
    "    - seasons_data (list): A list of DataFrames containing data for each season.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of tuples containing training and testing sets for each season.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    model = LogisticRegression(random_state=3270, max_iter=1000, penalty = None, solver = 'lbfgs')\n",
    "    accuracies = []\n",
    "    for i in range(1, len(seasons_data)):\n",
    "        print(f'Testing on Season {seasons_data[i][\"Season\"].unique()[0]}')\n",
    "        train = pd.concat(seasons_data[:i])\n",
    "        train = train.dropna(axis = 'columns', how= 'any')\n",
    "        X_train = train.drop(['Season','DayNum', 'team_1_TeamID', 'team_2_TeamID', 'team_1_won'], axis=1)\n",
    "        y_train = train['team_1_won']\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        test = seasons_data[i]\n",
    "        test = test.dropna(axis = 'columns', how= 'any')\n",
    "        X_test = test.drop(['Season','DayNum','team_1_TeamID', 'team_2_TeamID', 'team_1_won'], axis=1)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        y_test = test['team_1_won']\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'accuracy: {accuracy:.5f}')\n",
    "\n",
    "    print(f'Average accuracy: {np.mean(accuracies):.5f}')\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesSplit_by_season(seasons_matchup_avgs_10_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_matchups = seasons_matchup_avgs_10_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m y_test \u001b[38;5;241m=\u001b[39m last_season[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_1_won\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m---> 24\u001b[0m fitted \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfitted\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m fitted\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(60), (30,15), (60,15), (60,30,15)], \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['sgd', 'adam'], \n",
    "    'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "model = MLPClassifier(random_state=3270)\n",
    "scaler = StandardScaler()\n",
    "grid_search = GridSearchCV(model, param_grid, n_jobs=-1, verbose=2)\n",
    "\n",
    "all_except_last_season = pd.concat(season_matchups[:len(season_matchups)-1])\n",
    "last_season = pd.DataFrame(season_matchups[-1])\n",
    "X_train = all_except_last_season.drop(['Season','DayNum', 'team_1_TeamID', 'team_2_TeamID', 'team_1_won'], axis=1)\n",
    "y_train_ = all_except_last_season['team_1_won']\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = last_season.drop(['Season','DayNum','team_1_TeamID', 'team_2_TeamID', 'team_1_won'], axis=1)\n",
    "y_test = last_season['team_1_won']\n",
    "X_test = scaler.transform(X_test)\n",
    "fitted = grid_search.fit(X_train, y_train_)\n",
    "print(f'Best parameters: {fitted.best_params_}')\n",
    "y_pred = fitted.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy: {accuracy:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_tourney_data = []\n",
    "for season in range(2003 , 2025):\n",
    "    season_games = pd.read_csv(f'{season}/MNCAATourneyCompactResults_{season}.csv')\n",
    "    seasons_tourney_data.append(season_games)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_season = {}\n",
    "for season in merged_stats['Season'].unique():\n",
    "    season_stats = merged_stats[merged_stats['Season'] == season]\n",
    "    team_stats_season[season] = season_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_game_matchups(bracket_team_matchups, team_stats):\n",
    "    \"\"\"\n",
    "    Builds matchup data for the tournament bracket.\n",
    "\n",
    "    Parameters:\n",
    "    - bracket_team_matchups (DataFrame): The DataFrame containing the current matchups for a bracket.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame where each row contains team_1's and team_2's stats for that specific matchup.\n",
    "    \"\"\"\n",
    "    matchups = []\n",
    "    for _, row in bracket_team_matchups.iterrows():\n",
    "        team_1 = row['StrongSeed']\n",
    "        team_2 = row['WeakSeed']\n",
    "\n",
    "        team_1_data = team_stats[team_stats['TeamID'] == team_1]\n",
    "        team_2_data = team_stats[team_stats['TeamID'] == team_2]\n",
    "        \n",
    "        team_1_data.columns = [f'team_1_{col}' for col in team_1_data.columns]\n",
    "        team_2_data.columns = [f'team_2_{col}' for col in team_2_data.columns]\n",
    "        \n",
    "        team_1_data = team_1_data.reset_index(drop=True)\n",
    "        team_2_data = team_2_data.reset_index(drop=True)\n",
    "        matchup_data = pd.concat([team_1_data, team_2_data], axis=1)\n",
    "        matchup_data['Slot'] = row['Slot']\n",
    "        matchups.append(matchup_data)\n",
    "    return pd.concat(matchups, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_playins(seeds_df):\n",
    "    \"\"\"\n",
    "    Preprocess the play in teams by removing 'a' and 'b' designations and preparing strong and weak seeds.\n",
    "    \n",
    "    Parameters:\n",
    "    - seeds_df (DataFrame): The DataFrame containing tournament seeds data, including 'Seed', 'TeamID' columns.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame with 'StrongSeed' and 'WeakSeed' for play-in teams.\n",
    "    \"\"\"\n",
    "    playin_teams = seeds_df[seeds_df['Seed'].str.contains('a') | seeds_df['Seed'].str.contains('b')].copy()\n",
    "    playin_teams['Seed'] = playin_teams['Seed'].str.extract('([0-9A-Z]+)')\n",
    "    playin_teams_match_df = playin_teams.groupby('Seed')['TeamID'].apply(list).reset_index()\n",
    "    playin_teams_match_df['StrongSeed'] = playin_teams_match_df['TeamID'].apply(lambda x: x[0])\n",
    "    playin_teams_match_df['WeakSeed'] = playin_teams_match_df['TeamID'].apply(lambda x: x[1])\n",
    "    playin_teams_match_df.rename(columns={'Seed': 'Slot'}, inplace=True)\n",
    "    return playin_teams_match_df.drop(columns='TeamID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bracket_winners(bracket_matchups, model, scalar):\n",
    "    \"\"\"\n",
    "    Predict the winners in the lower bracket using a pre-trained model and scaler.\n",
    "    \n",
    "    Parameters:\n",
    "    - bracket_matchups (DataFrame): DataFrame of matchups in the lower bracket, excluding 'Seed' from scaling.\n",
    "    - model (Model): Pre-trained prediction model.\n",
    "    - scalar (Scaler): Pre-fitted scaler object for normalizing data.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Lower bracket DataFrame with an additional column 'team_1_won' indicating the predicted winner.\n",
    "    \"\"\"\n",
    "    bracket_matchups = bracket_matchups.drop(columns= [])\n",
    "    lower_bracket_scaled = scalar.fit_transform(bracket_matchups.drop(columns=['Slot']))\n",
    "    bracket_matchups['team_1_won'] = model.predict(lower_bracket_scaled)\n",
    "    return bracket_matchups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_seeds_with_winners(bracket, seeds_df):\n",
    "    \"\"\"\n",
    "    Update the seeds DataFrame with the winners from the lower bracket predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - bracket (DataFrame): The lower bracket DataFrame with predictions.\n",
    "    - seeds_df (DataFrame): The original seeds DataFrame to be updated with current teams seedings.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Updated seeds DataFrame with winners.\n",
    "    \"\"\"\n",
    "    bracket_winners = {}\n",
    "    for _, row in bracket.iterrows():\n",
    "        slot = row['Slot']\n",
    "        if slot not in bracket_winners:\n",
    "            bracket_winners[slot] = []\n",
    "        bracket_winners[slot].append(row['team_1_TeamID'] if row['team_1_won'] == 1 else row['team_2_TeamID'])\n",
    "    \n",
    "    for curr_seed, team in bracket_winners.items():\n",
    "        seeds_df.loc[len(seeds_df.index)] = [curr_seed, team[0]]\n",
    "    seeds_df.sort_values(by='Seed', inplace=True)\n",
    "    return seeds_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_round_matchups(team_seeds, round_slots):\n",
    "    \"\"\"\n",
    "    Build the Tourney round matchups based on seeds and updates team slots.\n",
    "    \n",
    "    Parameters:\n",
    "    - team_seeds (DataFrame): DataFrame containing the seeds and corresponding team IDs.\n",
    "    - round_slots (DataFrame): DataFrame containing the slots for the tournament matchups.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: First-round matchups with updated team slots based on seeds.\n",
    "    \"\"\"\n",
    "\n",
    "    for index, row in round_slots.iterrows():\n",
    "        strong_team = team_seeds[(team_seeds['Seed'] == row['StrongSeed'])]['TeamID'].values[0]\n",
    "        weak_team = team_seeds[(team_seeds['Seed'] == row['WeakSeed'])]['TeamID'].values[0]\n",
    "        round_slots.at[index, 'StrongSeed'] = strong_team\n",
    "        round_slots.at[index, 'WeakSeed'] = weak_team\n",
    "\n",
    "    return round_slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_seeds_df = pd.read_csv('2023/MNCAATourneySeeds_2023.csv')\n",
    "tourney_seeds_df.drop(columns=['Season'], inplace=True)\n",
    "tourney_slots_df = pd.read_csv('2023/MNCAATourneySlots_2023.csv')\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "playin_teams_match_df = preprocess_playins(tourney_seeds_df)\n",
    "bracket_matchups = build_game_matchups(playin_teams_match_df, team_stats_season[2023].groupby('TeamID').last().reset_index())\n",
    "bracket_matchups.columns\n",
    "bracket_matchups = predict_bracket_winners(bracket_matchups, model, scalar)\n",
    "tourney_seeds_df = update_seeds_with_winners(bracket_matchups, tourney_seeds_df)\n",
    "print('Done with play-ins')\n",
    "\n",
    "\n",
    "# curr_round_slots = tourney_slots_df[tourney_slots_df['Slot'].str.contains('R1')]\n",
    "\n",
    "# round_df = build_round_matchups(tourney_seeds_df,curr_round_slots)\n",
    "\n",
    "# current_round_bracket = build_game_matchups(round_df, team_stats_season[2023])\n",
    "# current_round_bracket.reset_index(drop=True, inplace=True)\n",
    "# current_round_bracket = predict_bracket_winners(current_round_bracket, model, scalar)\n",
    "# tourney_seeds_df = update_seeds_with_winners(current_round_bracket, tourney_seeds_df)\n",
    "\n",
    "\n",
    "rounds = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6']\n",
    "# rounds = ['R1']\n",
    "matchups = []\n",
    "for current_round in rounds:\n",
    "    curr_round_slots = tourney_slots_df[tourney_slots_df['Slot'].str.contains(current_round)]\n",
    "\n",
    "    round_matchups = build_round_matchups(tourney_seeds_df,curr_round_slots)\n",
    "    matchups.append(round_matchups)\n",
    "    current_round_bracket = build_game_matchups(round_matchups, team_stats_season[2023].groupby('TeamID').last().reset_index())\n",
    "    current_round_bracket = predict_bracket_winners(current_round_bracket, model, scalar)\n",
    "    tourney_seeds_df = update_seeds_with_winners(current_round_bracket, tourney_seeds_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_bracket = pd.concat(matchups)\n",
    "complete_bracket.reset_index(drop=True, inplace=True)\n",
    "# complete_bracket.to_csv('2023/MNCAATourneyPredictions_matchup_2023.csv', index=False)\n",
    "complete_bracket\n",
    "\n",
    "# tourney_seeds_df\n",
    "# test_dict = {}\n",
    "# test_dict[type(lgr).__name__] = [accuracy_score(y_test, y_pred)]\n",
    "# test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs3270p1_the_overfitting_overlords_train_save_load_all_models as tsm\n",
    "import os\n",
    "\n",
    "\n",
    "models = tsm.load_models()\n",
    "for model_name, model in models['avg_10'].items():\n",
    "    print(model_name)\n",
    "    print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_dir = 'data/Mens/Season/2024/predictions'\n",
    "prediction_file = 'MNCAATourneyPredictions_matchups_2024_rating_rol10_LogisticRegression.csv'\n",
    "seeds_df = 'MNCAATourneyPredictions__seeds_2024_rating_rol10_LogisticRegression.csv'\n",
    "\n",
    "matchups_df = pd.read_csv(os.path.join(prediction_dir, prediction_file))\n",
    "seeds_df = pd.read_csv(os.path.join(prediction_dir, seeds_df)).head(63)\n",
    "matchups_df.drop(columns=['Season'], inplace=True)\n",
    "seeds_df.rename(columns={'TeamID': 'Winner'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(matchups_df,seeds_df , left_on='Slot', right_on='Seed', how='left')\n",
    "merged_df.drop(columns=['Seed'], inplace=True)\n",
    "\n",
    "merged_df\n",
    "## Get all files in a dir\n",
    "files = os.listdir(prediction_dir)\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    if \"matchups\" in file:\n",
    "        curr_matchups = pd.read_csv(os.path.join(prediction_dir, file))\n",
    "        seed_file = file.replace('matchups', '_seeds')\n",
    "        curr_seeds = pd.read_csv(os.path.join(prediction_dir, seed_file))\n",
    "        curr_matchups.drop(columns=['Season'], inplace=True)\n",
    "        model_name = file.split('2024_')[-1].split('.')[0]\n",
    "        curr_matchups.rename(columns = {'StrongSeed' : 'StrongSeed_'+ model_name, 'WeakSeed' : 'WeakSeed_'+ model_name}, inplace=True)\n",
    "        curr_seeds.rename(columns = {'TeamID' : 'Winner_'+ model_name}, inplace=True)\n",
    "\n",
    "        curr_merge = pd.merge(curr_matchups, curr_seeds, left_on='Slot', right_on='Seed', how='left')\n",
    "        curr_merge.drop(columns=['Seed', 'Slot'], inplace=True)\n",
    "\n",
    "        merged_df = pd.concat([merged_df, curr_merge], axis=1)\n",
    "\n",
    "\n",
    "teams_names_df = pd.read_csv('data/Mens/MTeams.csv')\n",
    "\n",
    "team_names_map = teams_names_df.set_index('TeamID')['TeamName'].to_dict()\n",
    "\n",
    "for col in merged_df.columns[1:]:\n",
    "    merged_df[col] = merged_df[col].map(team_names_map)\n",
    "    \n",
    "# for index, row in pred_df.iterrows():\n",
    "#     strong_team = teams_names_df[teams_names_df['TeamID'] == row['StrongSeed']]['TeamName'].values[0]\n",
    "#     weak_team = teams_names_df[teams_names_df['TeamID'] == row['WeakSeed']]['TeamName'].values[0]\n",
    "#     pred_df.at[index, 'StrongSeed'] = strong_team\n",
    "#     pred_df.at[index, 'WeakSeed'] = weak_team\n",
    "\n",
    "merged_df.to_csv(os.path.join(prediction_dir,'MNCAATourneyPredictions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tourney_seeds_df.head(63))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv('data/Mens/Season/2023/MRegularSeasonDetailedResults_2023_avg_10_games.csv')\n",
    "stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv(f'data/Mens/Season/2023/MRegularSeasonDetailedResults_2023_avg_w_rating.csv')\n",
    "stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv(f'data/Mens/Season/2023/MRegularSeasonDetailedResults_2023_avg.csv')\n",
    "stats.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
